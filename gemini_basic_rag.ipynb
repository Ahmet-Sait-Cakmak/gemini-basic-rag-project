{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98328d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF Yüklemesinin Gerçekleştirilmesi\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"why-language-models-hallucinate.pdf\")\n",
    "data = loader.load()\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c465a697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veriyi parçalara ayırma(Chunking İşlemi)\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "## Chunk_Size 1000 demek her parçanın maksimum 1000 karakter uzunluğunda olacağını belirtir.\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(data)\n",
    "\n",
    "print(f\"Number of documents after chunking: {len(docs)}\")\n",
    "\n",
    "docs[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134ffd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Generative AI Embeddings'i kullanarak Embedding Oluşturma İşlemi\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "vector = embeddings.embed_query(\"hello, world!\")\n",
    "vector[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23fb3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChromaDB Üzerine Kayıt İşlemi\n",
    "vectorstore = Chroma.from_documents(documents=docs, embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\", search_kwargs={\"k\": 10}\n",
    ")\n",
    "retieved_docs = retriever.invoke(\"Why do LLMs hallucinate?\")\n",
    "len(retieved_docs)\n",
    "print(retieved_docs[5].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a738e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Gemini API Yapısını Kullanarak LLM Tetikleme İşlemleri\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-pro\",\n",
    "    temperature=0.3,\n",
    "    max_tokens=500\n",
    ")\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "system_prompt = (\n",
    "    \"You are assistant for question-answering tasks\"\n",
    "    \"Use the following pieces of retrieved context to answer\"\n",
    "    \"If you dont't know the answer, say that you don't know\"\n",
    "    \"Use three sentences maximum and keep the answer concise\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",system_prompt),\n",
    "        (\"human\",\"{input}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d90cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soru-Cevap Zinciri Oluşturma(LLM+PROMPT)\n",
    "question_answer_chain = create_stuff_documents_chain(llm,prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda98edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG Zinciri Oluşturma(RAG+LLM Entegrasyonun Gerçekleşmesi)\n",
    "rag_chain = create_retrieval_chain(retriever,question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee64f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kullanıcı Sorgusunu Çaıştırarak Cevap Üretme İşlemi\n",
    "response = rag_chain.invoke({\"input\":\"Why do LLMs hallucinate?\"})\n",
    "print(response)\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d20bf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performans Maetriklerinin Hesaplanması\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8195ea8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTIONS = [\n",
    "    \"Why do large language models hallucinate?\",\n",
    "    \"What causes hallucination in LLMs?\",\n",
    "    \"How does training data affect hallucinations?\",\n",
    "    \"What is the role of retrieval in reducing hallucination?\",\n",
    "    \"Why are LLMs confident even when wrong?\",\n",
    "    \"How can hallucinations be mitigated?\",\n",
    "    \"What is hallucination in NLP?\",\n",
    "    \"Why do generative models fabricate information?\",\n",
    "    \"What are common sources of hallucination?\",\n",
    "    \"How does RAG help reduce hallucinations?\"\n",
    "]\n",
    "\n",
    "GROUND_TRUTHS = [\n",
    "    \"LLMs hallucinate because they generate text based on probabilities rather than verified facts.\",\n",
    "    \"Hallucination occurs due to model uncertainty, lack of grounding, and training data limitations.\",\n",
    "    \"Incomplete, noisy, or biased training data can cause hallucinations.\",\n",
    "    \"Retrieval provides grounded context from documents, reducing hallucinations.\",\n",
    "    \"LLMs optimize fluency, not truth, which leads to confident but incorrect answers.\",\n",
    "    \"Hallucinations can be mitigated using retrieval, grounding, and better prompts.\",\n",
    "    \"Hallucination is the generation of incorrect or fabricated information.\",\n",
    "    \"Generative models predict likely text even when factual knowledge is missing.\",\n",
    "    \"Common sources include data gaps, ambiguity, and prompt design.\",\n",
    "    \"RAG injects external knowledge, grounding the model’s answers.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17095836",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for q, gt in zip(QUESTIONS, GROUND_TRUTHS):\n",
    "    # 1) RAG zincirini 1 kez çalıştır\n",
    "    resp = rag_chain.invoke({\"input\": q})\n",
    "\n",
    "    # 2) Model cevabını al\n",
    "    answer = resp[\"answer\"] if isinstance(resp, dict) and \"answer\" in resp else str(resp)\n",
    "\n",
    "    # 3) Aynı çalıştırmadan context’i al (mismatch olmaz)\n",
    "    contexts = [d.page_content for d in resp.get(\"context\", [])]\n",
    "\n",
    "    if not contexts:\n",
    "    contexts = [d.page_content for d in retriever.invoke(q)]\n",
    "\n",
    "    # 4) RAGAS formatında kayıt\n",
    "    results.append({\n",
    "        \"question\": q,\n",
    "        \"answer\": answer,\n",
    "        \"ground_truths\": [gt],\n",
    "        \"contexts\": contexts\n",
    "    })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf5519f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_list(results)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3431c197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- RAGAS: Hakem LLM + Embedding'i açıkça Gemini/Google olarak ayarla ---\n",
    "\n",
    "# Senin zaten kullandığın Gemini LLM nesnesi: llm\n",
    "# (ör: llm = ChatGoogleGenerativeAI(...))\n",
    "\n",
    "# Senin zaten kullandığın embedding nesnesi: embeddings\n",
    "# (ör: embeddings = GoogleGenerativeAIEmbeddings(...))\n",
    "\n",
    "try:\n",
    "    # Yeni sürümlerde genelde bu importlar çalışır\n",
    "    from ragas.llms import LangchainLLMWrapper\n",
    "    from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "\n",
    "    ragas_llm = LangchainLLMWrapper(llm)\n",
    "    ragas_embeddings = LangchainEmbeddingsWrapper(embeddings)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Wrapper importu başarısız. RAGAS sürümün farklı olabilir.\")\n",
    "    print(\"Hata:\", e)\n",
    "    ragas_llm = None\n",
    "    ragas_embeddings = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732cd371",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ragas_llm is not None and ragas_embeddings is not None:\n",
    "    report = evaluate(\n",
    "        dataset,\n",
    "        metrics=[faithfulness, answer_relevancy, context_precision, context_recall],\n",
    "        llm=ragas_llm,\n",
    "        embeddings=ragas_embeddings\n",
    "    )\n",
    "else:\n",
    "    # Wrapper yoksa eski davranışla dener (bazı sistemlerde yine çalışabilir)\n",
    "    report = evaluate(\n",
    "        dataset,\n",
    "        metrics=[faithfulness, answer_relevancy, context_precision, context_recall]\n",
    "    )\n",
    "\n",
    "\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c282a321",
   "metadata": {},
   "outputs": [],
   "source": [
    "report.to_pandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
